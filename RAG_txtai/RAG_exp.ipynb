{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17b042d-8cca-4422-b9d5-39d672604c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "from txtai.app import Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7541024-1b87-42a2-8daf-7e8a653dbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Application.read(\"./app.yml\")\n",
    "embeddings = Embeddings(config[\"embeddings\"], content=True)\n",
    "\n",
    "# Can also use the config programmatically\n",
    "#embeddings = Embeddings({\n",
    "#    \"path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#    \"backend\": \"qdrant.Qdrant\",\n",
    "#})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4e57c-1272-4c5d-ae64-2105729caaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.pipeline import LLM\n",
    "\n",
    "# Create LLM\n",
    "llm = LLM(\"TheBloke/Mistral-7B-OpenOrca-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf89a8-f661-42d0-bd5c-fd5f597bb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.pipeline import Textractor\n",
    "\n",
    "# Document text extraction, split into paragraphs\n",
    "textractor = Textractor(paragraphs=True)\n",
    "\n",
    "def stream(path):\n",
    "  for f in sorted(os.listdir(path)):\n",
    "    fpath = os.path.join(path, f)\n",
    "\n",
    "    # Only accept documents\n",
    "    if f.endswith((\"docx\", \"xlsx\", \"pdf\")):\n",
    "      print(f\"Indexing {fpath}\")\n",
    "      for paragraph in textractor(fpath):\n",
    "        yield paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1eca84-5380-42a3-8362-82b89457ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index(stream(\"docs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9d6ec-cc7c-4fd8-bf28-7bc379a4fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(question, text):\n",
    "  prompt = f\"\"\"<|im_start|>system\n",
    "  You are a friendly assistant. You answer questions from users.<|im_end|>\n",
    "  <|im_start|>user\n",
    "  Answer the following question using only the context below. Only include information specifically discussed.\n",
    "\n",
    "  question: {question}\n",
    "  context: {text} <|im_end|>\n",
    "  <|im_start|>assistant\n",
    "  \"\"\"\n",
    "\n",
    "  return llm(prompt, maxlength=4096, pad_token_id=32000)\n",
    "\n",
    "def context(prompt, ann_embeddings):\n",
    "  context =  \"\\n\".join(x[\"text\"] for x in )\n",
    "  return context\n",
    "\n",
    "def rag(prompt):\n",
    "  ann_embeddings = embeddings.search(prompt)\n",
    "  return prompt_llm(prompt, context(prompt, ann_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f17e1-1463-48d5-9d9f-97e585e32d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag(\"When was the BLIP model added for image captioning?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe83ec7-1e3a-4f48-94e6-70c96848d37e",
   "metadata": {},
   "source": [
    "# Citations in RAG\n",
    "\n",
    "We can also build pipelines that provide citations for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8a739-3287-461e-afa0-d0a0be5c3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.pipeline import Extractor\n",
    "\n",
    "# Extractor prompt\n",
    "def prompt(question):\n",
    "  return [{\n",
    "    \"query\": question,\n",
    "    \"question\": f\"\"\"\n",
    "Answer the following question using only the context below. Only include information specifically discussed.\n",
    "\n",
    "question: {question}\n",
    "context:\n",
    "\"\"\"\n",
    "}]\n",
    "\n",
    "# Create LLM with system prompt template\n",
    "llm = LLM(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", template=\"\"\"<|im_start|>system\n",
    "You are a friendly assistant. You answer questions from users.<|im_end|>\n",
    "<|im_start|>user\n",
    "{text} <|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\")\n",
    "\n",
    "# Create extractor instance\n",
    "extractor = Extractor(embeddings, llm, output=\"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691163f-d8c7-4f6d-bb39-58d75e8f4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extractor(prompt(\"What version of Python is supported?\"), maxlength=4096, pad_token_id=32000)[0]\n",
    "print(\"ANSWER:\", result[\"answer\"])\n",
    "print(\"CITATION:\", embeddings.search(\"select id, text from txtai where id = :id\", limit=1, parameters={\"id\": result[\"reference\"]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
